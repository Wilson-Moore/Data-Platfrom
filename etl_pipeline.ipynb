{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364d5e2d",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270fb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from PIL import Image,ImageEnhance\n",
    "import pytesseract\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1419b07",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c89ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_erp(host: str,database: str,user: str,password: str) -> None:\n",
    "    try:\n",
    "        connection=mysql.connector.connect(host=host,database=database,user=user,password=password)\n",
    "        \n",
    "        dataframe=pd.read_sql(\"SHOW TABLES\",connection)\n",
    "    \n",
    "        for column in dataframe.columns:\n",
    "            for table in dataframe[column]:\n",
    "                tmp=pd.read_sql(f\"SELECT * FROM {table}\",connection)\n",
    "                tmp.to_csv(f\"staging/{table}.csv\",index=False)\n",
    "        \n",
    "        connection.close()\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\"Error extracting database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ebab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ocr() -> None:\n",
    "    path = \"./data/legacy_invoices\"\n",
    "    orders_data=[]\n",
    "    files=[file for file in os.listdir(path) if file.lower().startswith(\"order\")]\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        image=Image.open(f\"{path}/{file}\")\n",
    "\n",
    "        enhancer=ImageEnhance.Contrast(image)\n",
    "        image=enhancer.enhance(1)\n",
    "        enhancer=ImageEnhance.Sharpness(image)\n",
    "        image=enhancer.enhance(2)\n",
    "\n",
    "        text=pytesseract.image_to_string(image,config=\"--psm 6 -c preserve_interword_spaces=1\")\n",
    "\n",
    "        data=parse_text(text.replace('$','S'))\n",
    "        orders_data.append(data)\n",
    "    \n",
    "    dataframe=pd.DataFrame(orders_data)\n",
    "    dataframe.to_csv(f\"staging/invoices.csv\",index=False)\n",
    "\n",
    "def parse_text(text: str) -> Dict:\n",
    "    data={}\n",
    "\n",
    "    order=re.search(r\"Ref[:,\\.]?\\s*([A-Z]+-\\d+)\",text)\n",
    "    data[\"Order_ID\"]=order.group(1)\n",
    "    date=re.search(r\"Date[:,\\.]?\\s*(\\d{4}-\\d{2}-\\d{2})\",text)\n",
    "    data[\"Date\"]=date.group(1)\n",
    "    client_id=re.search(r\"Client ID[:,\\.]?\\s*([A-Z]\\d+)\",text)\n",
    "    data[\"Customer_ID\"]=client_id.group(1)\n",
    "    client=re.search(r\"Nom[:,\\.]?\\s*([A-Z-a-z]+\\s[A-Z-a-z]+)\",text)\n",
    "    data[\"Full_Name\"]=client.group(1)\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line=line.strip()\n",
    "        \n",
    "        if not line or 'Produit' in line or '---' in line or 'Signature' in line:\n",
    "            continue\n",
    "        \n",
    "        if re.search(r'\\d+.*\\d+.*\\d+',line) and not any(keyword in line for keyword in ['Date','Ref','Client ID','Nom']):            \n",
    "            parts=re.split(r'\\s{2,}',line)\n",
    "            \n",
    "            if len(parts)>=4:\n",
    "                data['Product_Name']=parts[0]\n",
    "                data['Qte']=parts[1]\n",
    "                data['Unit_Price']=parts[2].replace(' ', '')\n",
    "                data['Total']=parts[3].replace(' ', '')\n",
    "            elif len(parts)>0:\n",
    "                product_match=re.search(r'([A-Za-zÀ-ÿ\\s\\d]+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)',line)\n",
    "                if product_match:\n",
    "                    data['Product_Name']=product_match.group(1).strip()\n",
    "                    data['Qte']=product_match.group(2)\n",
    "                    data['Unit_Price']=product_match.group(3)\n",
    "                    data['Total']=product_match.group(4)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ae0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_locale() -> None:\n",
    "    input_folder = Path(\"./data\")\n",
    "    output_folder = Path(\"./staging\")\n",
    "\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = [\n",
    "        \"marketing_expenses.xlsx\",\n",
    "        \"monthly_targets.xlsx\",\n",
    "        \"shipping_rates.xlsx\"\n",
    "    ]\n",
    "\n",
    "    for file_name in files:\n",
    "        file_path = input_folder / file_name\n",
    "\n",
    "        if not file_path.exists():\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            out_name = file_name.replace(\".xlsx\", \".csv\")\n",
    "            out_path = output_folder / out_name\n",
    "\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3f8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scraper(base_url: str) -> None:\n",
    "    all_products=[]\n",
    "    all_prices=[]\n",
    "\n",
    "    current_url=base_url\n",
    "\n",
    "    while current_url:\n",
    "\n",
    "        try:\n",
    "            res=requests.get(current_url)\n",
    "            soup=BeautifulSoup(res.content,\"html.parser\")\n",
    "\n",
    "\n",
    "            for tag in soup.find_all([\"h5\"],class_=[\"product-name\"]):\n",
    "                all_products.append(tag.text)\n",
    "            for tag in soup.find_all([\"span\"],class_=[\"product-price\"]):\n",
    "                all_prices.append(str(tag.text).replace(\"DZD\",\"\"))\n",
    "\n",
    "            next_link=soup.find([\"a\"],id=\"next-page-btn\")\n",
    "            \n",
    "            if next_link and next_link.get(\"href\"):\n",
    "                current_url=\"\".join([base_url,\"/\",next_link.get(\"href\")])\n",
    "            else:\n",
    "                current_url=None\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error scraping {current_url}: {e}\")\n",
    "            break\n",
    "\n",
    "    dataframe=pd.DataFrame({\n",
    "        \"Product_Name\": all_products,\n",
    "        \"Unit_Price\": all_prices\n",
    "    })\n",
    "    \n",
    "    dataframe.to_csv(\"staging/competitor.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5e7ff",
   "metadata": {},
   "source": [
    "Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USD_TO_DZD = 135.0\n",
    "\n",
    "def handle_neg_values(df, column_names = []) -> pd.DataFrame:\n",
    "    for column in column_names:\n",
    "        df[column] = df[column].apply(lambda x: x if x >= 0 else None)\n",
    "    return df\n",
    "\n",
    "def clean_id(df, column_name) -> pd.DataFrame:\n",
    "    df[column_name] = df[column_name].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def clean_date(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    s = df[column_name].astype(str).str.strip()\n",
    "\n",
    "    # normalize separators: 2021.02.28 -> 2021-02-28\n",
    "    s = s.str.replace(r\"\\.\", \"-\", regex=True)\n",
    "\n",
    "    parsed = pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "    # YYYY-MM-DD (after dot->dash normalization)\n",
    "    mask = parsed.isna()\n",
    "    parsed_y = pd.to_datetime(s[mask], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    parsed.loc[mask] = parsed_y\n",
    "\n",
    "    # MM-DD-YYYY\n",
    "    mask = parsed.isna()\n",
    "    parsed_mdy = pd.to_datetime(s[mask], format=\"%m-%d-%Y\", errors=\"coerce\")\n",
    "    parsed.loc[mask] = parsed_mdy\n",
    "\n",
    "    # Month Day, Year (March 3, 2021)\n",
    "    mask = parsed.isna()\n",
    "    parsed_long = pd.to_datetime(s[mask], format=\"%B %d, %Y\", errors=\"coerce\")\n",
    "    parsed.loc[mask] = parsed_long\n",
    "\n",
    "    # Mon-YYYY (Feb-2023) -> 2023-02-01\n",
    "    mask = parsed.isna()\n",
    "    parsed_mon_year = pd.to_datetime(s[mask], format=\"%b-%Y\", errors=\"coerce\")\n",
    "    parsed.loc[mask] = parsed_mon_year\n",
    "\n",
    "    df[column_name] = parsed\n",
    "    return df\n",
    "\n",
    "def usd_to_dzd(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    df[column_name] = df[column_name] * USD_TO_DZD\n",
    "\n",
    "    new_column_name = column_name.replace(\"USD\", \"DZD\").replace(\"usd\", \"dzd\")\n",
    "    df.rename(columns={column_name: new_column_name}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def standarize_names(df: pd.DataFrame, column_names=[]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for column in column_names:\n",
    "        df[column] = (\n",
    "            df[column]\n",
    "            .astype(str)\n",
    "            .str.replace(\"_\", \" \", regex=False)\n",
    "            .str.replace(\"-\", \" \", regex=False) \n",
    "            .str.strip()\n",
    "            .str.title()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_number(df: pd.DataFrame, column_names) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in column_names:\n",
    "        s = df[col].astype(\"string\")  # better than astype(str), preserves <NA>\n",
    "\n",
    "        # remove currency text/symbols if they exist (optional)\n",
    "        s = s.str.replace(\"USD\", \"\", regex=False).str.replace(\"$\", \"\", regex=False).str.strip()\n",
    "\n",
    "        # remove spaces (thousand sep)\n",
    "        s = s.str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "        # if decimal comma is used, convert it to dot\n",
    "        # example: \"1234,56\" -> \"1234.56\"\n",
    "        s = s.str.replace(\",\", \".\", regex=False)\n",
    "\n",
    "        df[col] = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "507b961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transform_marketing_expenses() -> None:\n",
    "    df = pd.read_csv(\"staging/marketing_expenses.csv\")\n",
    "\n",
    "    # 1) Clean date and prices\n",
    "    df = clean_date(df, \"Date\")\n",
    "    df = normalize_number(df, [\"Marketing_Cost_USD\"])\n",
    "\n",
    "    # 2) Add Month column (start of month)\n",
    "    df[\"Month\"] = df[\"Date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "    df[\"Month\"] = pd.to_datetime(df[\"Month\"], errors=\"coerce\")\n",
    "\n",
    "    # 5) Negatives -> NULL\n",
    "    df = handle_neg_values(df, [\"Marketing_Cost_USD\"])\n",
    "\n",
    "    # 7) Convert USD -> DZD and rename\n",
    "    df = usd_to_dzd(df, \"Marketing_Cost_USD\")  # creates Marketing_Cost_DZD\n",
    "\n",
    "    # 3) Standardize names\n",
    "    df = standarize_names(df, [\"Category\", \"Campaign_Type\"])\n",
    "\n",
    "    # 6) Fill NULL with avg of same (Category, Campaign_Type)\n",
    "    df[\"Marketing_Cost_DZD\"] = df[\"Marketing_Cost_DZD\"].fillna(\n",
    "        df.groupby([\"Category\", \"Campaign_Type\"])[\"Marketing_Cost_DZD\"].transform(\"mean\")\n",
    "    )\n",
    "\n",
    "    # 8) Add monthly average marketing for this Category (Month + Category)\n",
    "    df[\"Avg_Monthly_Category_Marketing_Cost\"] = df.groupby(\n",
    "        [\"Month\", \"Category\"]\n",
    "    )[\"Marketing_Cost_DZD\"].transform(\"mean\")\n",
    "\n",
    "    # 9) Remove duplicates\n",
    "    df = remove_duplicates(df)\n",
    "\n",
    "    df.to_csv(\"staging/marketing_expenses.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "def transform_cities() -> None:\n",
    "    cities = pd.read_csv(\"staging/table_cities.csv\")\n",
    "    shipping = pd.read_csv(\"staging/shipping_rates.csv\") \n",
    "\n",
    "    # average shipping cost per region\n",
    "    avg_shipping = (\n",
    "        shipping.groupby(\"region_name\", as_index=False)[\"shipping_cost\"]\n",
    "        .mean()\n",
    "        .rename(columns={\n",
    "            \"region_name\": \"Region\",\n",
    "            \"shipping_cost\": \"Avg_Region_Shipping_Cost\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # add Avg_Region_Shipping_Cost to cities by Region\n",
    "    cities = cities.merge(\n",
    "        avg_shipping,\n",
    "        on=\"Region\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    avg_value = cities[\"Avg_Region_Shipping_Cost\"].mean()\n",
    "    cities[\"Avg_Region_Shipping_Cost\"] = cities[\"Avg_Region_Shipping_Cost\"].fillna(avg_value)\n",
    "\n",
    "    cities.to_csv(\"staging/table_cities.csv\", index=False)\n",
    "\n",
    "def transform_monthly_targets() -> None:\n",
    "    df = pd.read_csv(\"staging/monthly_targets.csv\")\n",
    "\n",
    "    # 1) Clean Store_ID like: S1, Store_5 -> 1, 5 ...\n",
    "    df = clean_id(df, \"Store_ID\")\n",
    "\n",
    "    # 2) Clean Month (handles \"Feb-2023\", \"Apr-2023\", \"2023-01-01 00:00:00\", etc.)\n",
    "    df = clean_date(df, \"Month\")\n",
    "\n",
    "    # 3) Clean Target_Revenue (remove commas, spaces, etc.)\n",
    "    df = normalize_number(df, [\"Target_Revenue\"])\n",
    "\n",
    "    # 4) Fill null Target_Revenue with the average of the same store\n",
    "    # (if a store has all NaNs, this will remain NaN)\n",
    "    df[\"Target_Revenue\"] = df[\"Target_Revenue\"].fillna(\n",
    "        df.groupby(\"Store_ID\")[\"Target_Revenue\"].transform(\"mean\")\n",
    "    )\n",
    "    # 5) Standardize Manager_Name\n",
    "    df = standarize_names(df, [\"Manager_Name\"])\n",
    "\n",
    "    # 6) Remove duplicates\n",
    "    df = remove_duplicates(df)\n",
    "\n",
    "    df.to_csv(\"staging/monthly_targets.csv\", index=False)\n",
    "\n",
    "def transform_subcategories() -> None:\n",
    "    sub = pd.read_csv(\"staging/table_subcategories.csv\")\n",
    "    cat = pd.read_csv(\"staging/table_categories.csv\")\n",
    "\n",
    "    sub = sub.merge(\n",
    "        cat[[\"Category_ID\", \"Category_Name\"]],\n",
    "        on=\"Category_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # remove foreign key\n",
    "    sub.drop(columns=[\"Category_ID\"], inplace=True)\n",
    "\n",
    "    sub.to_csv(\"staging/table_subcategories.csv\", index=False)\n",
    "\n",
    "def fix_sales_ids() -> None:\n",
    "    sales = pd.read_csv(\"staging/table_sales.csv\")\n",
    "    sales = clean_id(sales, \"Trans_ID\")\n",
    "    sales.to_csv(\"staging/table_sales.csv\", index=False)\n",
    "\n",
    "def add_invoices() -> None:\n",
    "\n",
    "    sales = pd.read_csv(\"staging/table_sales.csv\")\n",
    "    invoices = pd.read_csv(\"staging/invoices.csv\")\n",
    "    products = pd.read_csv(\"staging/table_products.csv\")\n",
    "\n",
    "\n",
    "    invoices = invoices.merge(\n",
    "        products[[\"Product_ID\", \"Product_Name\"]],\n",
    "        on=\"Product_Name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "    existing_ids = pd.to_numeric(sales[\"Trans_ID\"], errors=\"coerce\")\n",
    "    max_id = int(existing_ids.max()) if existing_ids.notna().any() else 0\n",
    "\n",
    "    new_ids = pd.RangeIndex(start=max_id + 1, stop=max_id + 1 + len(invoices))\n",
    "    invoices[\"Trans_ID\"] = new_ids.astype(int)\n",
    "\n",
    "    # safety: ensure no collision (very unlikely, but guaranteed)\n",
    "    existing_set = set(pd.to_numeric(sales[\"Trans_ID\"], errors=\"coerce\").dropna().astype(int).tolist())\n",
    "    while invoices[\"Trans_ID\"].isin(existing_set).any():\n",
    "        max_id += len(invoices)\n",
    "        invoices[\"Trans_ID\"] = pd.RangeIndex(start=max_id + 1, stop=max_id + 1 + len(invoices)).astype(int)\n",
    "\n",
    "    # ---- rename / select columns to match sales ----\n",
    "    invoices = invoices.rename(columns={\n",
    "        \"Qte\": \"Quantity\",\n",
    "        \"Total\": \"Total_Revenue\",\n",
    "    })\n",
    "\n",
    "    invoices_sales = invoices[[\n",
    "        \"Trans_ID\",\n",
    "        \"Date\",\n",
    "        \"Customer_ID\",\n",
    "        \"Product_ID\",\n",
    "        \"Quantity\",\n",
    "        \"Total_Revenue\",\n",
    "    ]]\n",
    "\n",
    "    # ---- append into sales ----\n",
    "    sales = pd.concat([sales, invoices_sales], ignore_index=True)\n",
    "\n",
    "    sales.to_csv(\"staging/table_sales.csv\", index=False)\n",
    "\n",
    "def transform_products() -> None:\n",
    "    products = pd.read_csv(\"staging/table_products.csv\")\n",
    "    subcats = pd.read_csv(\"staging/table_subcategories.csv\")\n",
    "    competitor = pd.read_csv(\"staging/competitor.csv\")\n",
    "\n",
    "    products = clean_id(products, \"Product_ID\")\n",
    "\n",
    "    # 1) add SubCat_Name + Category_Name\n",
    "    products = products.merge(\n",
    "        subcats[[\"SubCat_ID\", \"SubCat_Name\", \"Category_Name\"]],\n",
    "        on=\"SubCat_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 2) remove foreign key\n",
    "    products.drop(columns=[\"SubCat_ID\"], inplace=True)\n",
    "\n",
    "    # 3) add competitor price (if exists)\n",
    "    products = products.merge(\n",
    "        competitor[[\"Product_Name\", \"Unit_Price\"]]\n",
    "            .rename(columns={\"Unit_Price\": \"Competitor_Unit_Price\"}),\n",
    "        on=\"Product_Name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    products.to_csv(\"staging/table_products.csv\", index=False)\n",
    "\n",
    "\n",
    "def transform_customers() -> None:\n",
    "    customers = pd.read_csv(\"staging/table_customers.csv\")\n",
    "    cities = pd.read_csv(\"staging/table_cities.csv\")\n",
    "\n",
    "    customers = clean_id(customers, \"Customer_ID\")\n",
    "\n",
    "    customers = customers.merge(\n",
    "        cities[[\"City_ID\", \"City_Name\", \"Region\", \"Avg_Region_Shipping_Cost\"]],\n",
    "        on=\"City_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # remove foreign key\n",
    "    customers.drop(columns=[\"City_ID\"], inplace=True)\n",
    "\n",
    "    customers.to_csv(\"staging/table_customers.csv\", index=False)\n",
    "\n",
    "\n",
    "def transform_table_stores() -> None:\n",
    "    stores = pd.read_csv(\"staging/table_stores.csv\")\n",
    "    cities = pd.read_csv(\"staging/table_cities.csv\")\n",
    "    targets = pd.read_csv(\"staging/monthly_targets.csv\")\n",
    "\n",
    "    # ---- add City_Name + Region ----\n",
    "    stores = stores.merge(\n",
    "        cities[[\"City_ID\", \"City_Name\", \"Region\"]],\n",
    "        on=\"City_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # ---- compute average monthly target per store ----\n",
    "    targets[\"Target_Revenue\"] = (\n",
    "        targets[\"Target_Revenue\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "    )\n",
    "    targets[\"Target_Revenue\"] = pd.to_numeric(targets[\"Target_Revenue\"], errors=\"coerce\")\n",
    "\n",
    "    avg_targets = (\n",
    "        targets.groupby(\"Store_ID\", as_index=False)[\"Target_Revenue\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"Target_Revenue\": \"Monthly_Target\"})\n",
    "    )\n",
    "\n",
    "    # ---- merge avg target into stores ----\n",
    "    stores = stores.merge(\n",
    "        avg_targets,\n",
    "        on=\"Store_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # ---- drop foreign key ----\n",
    "    stores.drop(columns=[\"City_ID\"], inplace=True)\n",
    "\n",
    "    stores.to_csv(\"staging/table_stores.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "def transform_sales() -> None:\n",
    "    sales = pd.read_csv(\"staging/table_sales.csv\")\n",
    "    products = pd.read_csv(\"staging/table_products.csv\")\n",
    "    customers = pd.read_csv(\"staging/table_customers.csv\")\n",
    "    marketing = pd.read_csv(\"staging/marketing_expenses.csv\", parse_dates=[\"Month\"])\n",
    "\n",
    "    sales = clean_id(sales, \"Customer_ID\")\n",
    "    sales = clean_id(sales, \"Product_ID\")\n",
    "    sales = clean_date(sales, \"Date\")\n",
    "\n",
    "    # 1) month of the sale\n",
    "    sales[\"Month\"] = pd.to_datetime(sales[\"Date\"]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    # 2) bring category + unit cost into sales\n",
    "    sales = sales.merge(\n",
    "        products[[\"Product_ID\", \"Unit_Cost\", \"Category_Name\"]],\n",
    "        on=\"Product_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 3) shipping cost directly from customers\n",
    "    sales = sales.merge(\n",
    "        customers[[\"Customer_ID\", \"Avg_Region_Shipping_Cost\"]],\n",
    "        on=\"Customer_ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    sales[\"Shipping_Cost\"] = sales[\"Avg_Region_Shipping_Cost\"]\n",
    "\n",
    "    # 4) marketing monthly by (Month + Category)\n",
    "    monthly_cat_marketing = marketing[[\"Month\", \"Category\", \"Avg_Monthly_Category_Marketing_Cost\"]]\n",
    "\n",
    "    sales = sales.merge(\n",
    "        monthly_cat_marketing,\n",
    "        left_on=[\"Month\", \"Category_Name\"],\n",
    "        right_on=[\"Month\", \"Category\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    sales[\"Marketing_Cost\"] = sales[\"Avg_Monthly_Category_Marketing_Cost\"].fillna(0)\n",
    "\n",
    "    # count of sales rows in same Month+Category\n",
    "    sales[\"cat_monthly_sales\"] = sales.groupby([\"Month\", \"Category_Name\"])[\"Product_ID\"].transform(\"count\")\n",
    "\n",
    "    # avoid division by zero\n",
    "    sales[\"cat_monthly_sales\"] = sales[\"cat_monthly_sales\"].replace(0, 1)\n",
    "\n",
    "    sales[\"Marketing_Cost\"] = sales[\"Marketing_Cost\"] / sales[\"cat_monthly_sales\"]\n",
    "\n",
    "    # 5) net profit (split marketing fairly)\n",
    "    sales[\"Net_Profit\"] = (\n",
    "        sales[\"Total_Revenue\"]\n",
    "        - (sales[\"Unit_Cost\"] * sales[\"Quantity\"])\n",
    "        - sales[\"Shipping_Cost\"]\n",
    "        - sales[\"Marketing_Cost\"]\n",
    "    )\n",
    "\n",
    "    sales.drop(columns=[\"Month\", \"Category\", \"Avg_Monthly_Category_Marketing_Cost\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    sales.to_csv(\"staging/table_sales.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "def review_text_to_score() -> None:\n",
    "    dataframe=pd.read_csv(\"staging/table_reviews.csv\")\n",
    "    sid_obj=SentimentIntensityAnalyzer()\n",
    "    \n",
    "    score=[]\n",
    "    \n",
    "    for product in dataframe.groupby(\"Product_ID\"):\n",
    "        count=0\n",
    "        sum=0\n",
    "        for sentence in product[1][\"Review_Text\"]:\n",
    "            sentiment_dict=sid_obj.polarity_scores(sentence)\n",
    "            count+=1\n",
    "            sum+=sentiment_dict['compound']\n",
    "        score.append(sum/count)\n",
    "    \n",
    "    dataframe=pd.read_csv(\"staging/table_products.csv\")\n",
    "    dataframe.sort_values(by=[\"Product_ID\"])\n",
    "    dataframe[\"Score\"]=score\n",
    "\n",
    "    dataframe.to_csv(\"staging/table_products.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "def transform_erp() -> None:\n",
    "    transform_marketing_expenses()\n",
    "    transform_cities()\n",
    "    transform_monthly_targets()\n",
    "    transform_subcategories()\n",
    "    fix_sales_ids()\n",
    "    add_invoices()\n",
    "    transform_products()\n",
    "    transform_customers()\n",
    "    transform_table_stores()\n",
    "    transform_sales()\n",
    "    review_text_to_score()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66708a",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49bf8559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data warehouse schema created (no data loaded).\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = Path(\"techstore_dw.db\")\n",
    "\n",
    "def create_dw_schema() -> None:\n",
    "    # recreate DB\n",
    "    if DB_PATH.exists():\n",
    "        DB_PATH.unlink()\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.executescript(\"\"\"\n",
    "    PRAGMA foreign_keys = ON;\n",
    "\n",
    "    -- =====================\n",
    "    -- DIMENSIONS\n",
    "    -- =====================\n",
    "\n",
    "    CREATE TABLE Dim_Product (\n",
    "        Product_ID            TEXT PRIMARY KEY,\n",
    "        Product_Name          TEXT,\n",
    "        SubCat_Name           TEXT,\n",
    "        Category_Name         TEXT,\n",
    "        Unit_Price            REAL,\n",
    "        Unit_Cost             REAL,\n",
    "        Score                 REAL,\n",
    "        Competitor_Unit_Price REAL\n",
    "    );\n",
    "\n",
    "    CREATE TABLE Dim_Store (\n",
    "        Store_ID           INTEGER PRIMARY KEY,\n",
    "        Store_Name         TEXT,\n",
    "        City_Name          TEXT,\n",
    "        Region             TEXT,\n",
    "        Monthly_Target REAL\n",
    "    );\n",
    "\n",
    "    CREATE TABLE Dim_Customer (\n",
    "        Customer_ID              TEXT PRIMARY KEY,\n",
    "        Full_Name                TEXT,\n",
    "        City_Name                TEXT,\n",
    "        Region                   TEXT,\n",
    "        Avg_Region_Shipping_Cost REAL\n",
    "    );\n",
    "\n",
    "    CREATE TABLE Dim_Date (\n",
    "        DateKey  INTEGER PRIMARY KEY, -- YYYYMMDD\n",
    "        Day      INTEGER,\n",
    "        Month    INTEGER,\n",
    "        Year     INTEGER,\n",
    "        DayName  TEXT\n",
    "    );\n",
    "\n",
    "    -- =====================\n",
    "    -- FACT TABLE\n",
    "    -- =====================\n",
    "\n",
    "    CREATE TABLE Fact_Sales (\n",
    "        Trans_ID      INTEGER PRIMARY KEY,\n",
    "        DateKey       INTEGER NOT NULL,\n",
    "        Store_ID      INTEGER NOT NULL,\n",
    "        Product_ID    TEXT NOT NULL,\n",
    "        Customer_ID   TEXT NOT NULL,\n",
    "\n",
    "        Quantity      INTEGER,\n",
    "        Total_Revenue REAL,\n",
    "        Net_Profit    REAL,\n",
    "        Marketing_Cost  REAL,\n",
    "\n",
    "        FOREIGN KEY (DateKey)    REFERENCES Dim_Date(DateKey),\n",
    "        FOREIGN KEY (Store_ID)   REFERENCES Dim_Store(Store_ID),\n",
    "        FOREIGN KEY (Product_ID) REFERENCES Dim_Product(Product_ID),\n",
    "        FOREIGN KEY (Customer_ID) REFERENCES Dim_Customer(Customer_ID)\n",
    "    );\n",
    "\n",
    "    -- =====================\n",
    "    -- INDEXES (performance)\n",
    "    -- =====================\n",
    "\n",
    "    CREATE INDEX idx_fact_date     ON Fact_Sales(DateKey);\n",
    "    CREATE INDEX idx_fact_store    ON Fact_Sales(Store_ID);\n",
    "    CREATE INDEX idx_fact_product  ON Fact_Sales(Product_ID);\n",
    "    CREATE INDEX idx_fact_customer ON Fact_Sales(Customer_ID);\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dw_schema()\n",
    "    print(\"✅ Data warehouse schema created (no data loaded).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c65e410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DW loaded into techstore_dw.db\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = Path(\"techstore_dw.db\")\n",
    "\n",
    "STAGING = Path(\"staging\")\n",
    "PRODUCTS_CSV   = STAGING / \"table_products.csv\"\n",
    "STORES_CSV     = STAGING / \"table_stores.csv\"\n",
    "CUSTOMERS_CSV  = STAGING / \"table_customers.csv\"\n",
    "SALES_CSV      = STAGING / \"table_sales.csv\"\n",
    "\n",
    "\n",
    "def load_dw_data() -> None:\n",
    "    # 1) create DB if not exists\n",
    "    if not DB_PATH.exists():\n",
    "        create_dw_schema()\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "    # 2) read staging\n",
    "    products  = pd.read_csv(PRODUCTS_CSV)\n",
    "    stores    = pd.read_csv(STORES_CSV)\n",
    "    customers = pd.read_csv(CUSTOMERS_CSV)\n",
    "    sales     = pd.read_csv(SALES_CSV)\n",
    "\n",
    "    # 3) build Dim_Date from sales Date\n",
    "    sales[\"Date\"] = pd.to_datetime(sales[\"Date\"], errors=\"coerce\", format=\"mixed\")\n",
    "    dates = pd.Series(sales[\"Date\"].dropna().dt.normalize().unique(), name=\"Date\")\n",
    "    dim_date = pd.DataFrame({\"Date\": dates})\n",
    "    dim_date[\"DateKey\"] = dim_date[\"Date\"].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    dim_date[\"Day\"] = dim_date[\"Date\"].dt.day\n",
    "    dim_date[\"Month\"] = dim_date[\"Date\"].dt.month\n",
    "    dim_date[\"Year\"] = dim_date[\"Date\"].dt.year\n",
    "    dim_date[\"DayName\"] = dim_date[\"Date\"].dt.strftime(\"%A\")\n",
    "    dim_date = dim_date.drop(columns=[\"Date\"])\n",
    "\n",
    "    sales[\"DateKey\"] = sales[\"Date\"].dt.strftime(\"%Y%m%d\").astype(\"Int64\")\n",
    "\n",
    "    # 4) clear old rows\n",
    "    cur.executescript(\"\"\"\n",
    "        DELETE FROM Fact_Sales;\n",
    "        DELETE FROM Dim_Date;\n",
    "        DELETE FROM Dim_Customer;\n",
    "        DELETE FROM Dim_Store;\n",
    "        DELETE FROM Dim_Product;\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    # 5) load dimensions\n",
    "    prod_cols = [\"Product_ID\",\"Product_Name\",\"SubCat_Name\",\"Category_Name\",\n",
    "                 \"Unit_Price\",\"Unit_Cost\",\"Score\",\"Competitor_Unit_Price\"]\n",
    "    products[[c for c in prod_cols if c in products.columns]].to_sql(\n",
    "        \"Dim_Product\", conn, if_exists=\"append\", index=False\n",
    "    )\n",
    "\n",
    "    store_cols = [\"Store_ID\",\"Store_Name\",\"City_Name\",\"Region\",\"Monthly_Target\"]\n",
    "    stores[[c for c in store_cols if c in stores.columns]].to_sql(\n",
    "        \"Dim_Store\", conn, if_exists=\"append\", index=False\n",
    "    )\n",
    "\n",
    "    cust_cols = [\"Customer_ID\",\"Full_Name\",\"City_Name\",\"Region\",\"Region_Shipping_Cost\"]\n",
    "    customers[[c for c in cust_cols if c in customers.columns]].to_sql(\n",
    "        \"Dim_Customer\", conn, if_exists=\"append\", index=False\n",
    "    )\n",
    "\n",
    "    dim_date.to_sql(\"Dim_Date\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    # 6) load fact\n",
    "    fact_cols = [\"Trans_ID\",\"DateKey\",\"Store_ID\",\"Product_ID\",\"Customer_ID\",\n",
    "                 \"Quantity\",\"Total_Revenue\",\"Net_Profit\", \"Marketing_Cost\"]\n",
    "    fact = sales[[c for c in fact_cols if c in sales.columns]].copy()\n",
    "    fact = fact.dropna(subset=[\"Trans_ID\",\"DateKey\",\"Store_ID\",\"Product_ID\",\"Customer_ID\"])\n",
    "    fact[\"DateKey\"] = fact[\"DateKey\"].astype(int)\n",
    "\n",
    "    fact.to_sql(\"Fact_Sales\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dw_data()\n",
    "    print(\"✅ DW loaded into techstore_dw.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfc59f",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c39330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(ENV_KEYS: Dict[str, str | None]) -> None:\n",
    "    print(\"EXTRACTING DATABASE\")\n",
    "    extract_erp(\n",
    "        ENV_KEYS.get(\"DB_HOST\"),\n",
    "        ENV_KEYS.get(\"DB_NAME\"),\n",
    "        ENV_KEYS.get(\"DB_USERNAME\"),\n",
    "        ENV_KEYS.get(\"DB_PASSWORD\")\n",
    "    )\n",
    "    print(\"DONE EXTRACTING DATABASE\")\n",
    "\n",
    "    print(\"EXTRACTING WEBSITE\")\n",
    "    extract_scraper(ENV_KEYS.get(\"WEBSITE\"))\n",
    "    print(\"DONE EXTRACTING WEBSITE\")\n",
    "\n",
    "    print(\"EXTRACTING LOCAL FILES\")\n",
    "    extract_locale()\n",
    "    print(\"DONE EXTRACTING LOCAL FILES\")\n",
    "\n",
    "    print(\"EXTRACTING IMAGES\")\n",
    "    extract_ocr()\n",
    "    print(\"DONE EXTRACTING LOCAL FILES\")\n",
    "\n",
    "def transfrom() -> None:\n",
    "    print(\"TRANSFROMING ERP\")\n",
    "    transform_erp()\n",
    "    print(\"DONE TRANSFROMING ERP\")\n",
    "\n",
    "def load():\n",
    "    print(\"LOADING DATA WAREHOUSE\")\n",
    "    # load dimensions + fact\n",
    "    load_dw_data()\n",
    "    print(\"DONE LOADING DATA WAREHOUSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d949afad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTING DATABASE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18084\\4067479274.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dataframe=pd.read_sql(\"SHOW TABLES\",connection)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18084\\4067479274.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  tmp=pd.read_sql(f\"SELECT * FROM {table}\",connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE EXTRACTING DATABASE\n",
      "EXTRACTING WEBSITE\n",
      "DONE EXTRACTING WEBSITE\n",
      "EXTRACTING LOCAL FILES\n",
      "DONE EXTRACTING LOCAL FILES\n",
      "EXTRACTING IMAGES\n",
      "DONE EXTRACTING LOCAL FILES\n",
      "TRANSFROMING ERP\n",
      "DONE TRANSFROMING ERP\n",
      "LOADING DATA WAREHOUSE\n",
      "DONE LOADING DATA WAREHOUSE\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "ENV_KEYS=dotenv_values(\".env\")\n",
    "\n",
    "extract(ENV_KEYS)\n",
    "\n",
    "transfrom()\n",
    "\n",
    "load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
